{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "f = open('data/mini_glove.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove2kSparse(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Glove2kSparse, self).__init__()\n",
    "        self.encoder = nn.Linear(100, 1000)\n",
    "        self.decoder = nn.Linear(1000, 100)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        topk = torch.topk(x, 7).indices\n",
    "        debug_count = 0\n",
    "        for index in range(len(x)):\n",
    "            if index not in topk:\n",
    "                x[index] = 0\n",
    "                debug_count += 1\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):  # Unsqueeze / squeeze?\n",
    "        return self.decoder(self.encode(x))\n",
    "    \n",
    "    def view_grads(self):\n",
    "        enc_grads = net.encoder.weight.grad.resize(100000)\n",
    "        dec_grads = net.decoder.weight.grad.resize(100000)\n",
    "        return sparse_vec_to_dict(torch.cat([enc_grads, dec_grads]))\n",
    "        \n",
    "net = Glove2kSparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000     0.19631743665784598\n",
      "2000     0.19271614674478768\n",
      "3000     0.19105273923277855\n",
      "4000     0.18984142135456206\n",
      "5000     0.1884737504310906\n",
      "6000     0.18714667668938637\n",
      "7000     0.1864700799568423\n",
      "8000     0.18611903664050625\n",
      "9000     0.18680317832570936\n",
      "Epoch 0 Average Loss:  0.184958866462484\n",
      "1000     0.1490420922189951\n",
      "2000     0.14962921951711178\n",
      "3000     0.15072916285072763\n",
      "4000     0.15079264387767763\n",
      "5000     0.15034850658252835\n",
      "6000     0.14982978206003705\n",
      "7000     0.14971965642486298\n",
      "8000     0.15013340625632554\n",
      "9000     0.1517731101397011\n",
      "Epoch 1 Average Loss:  0.15145419912971556\n",
      "1000     0.13440479839593172\n",
      "2000     0.1334765246156603\n",
      "3000     0.1334320896329979\n",
      "4000     0.13361731371469796\n",
      "5000     0.13379363585039974\n",
      "6000     0.13379630222978692\n",
      "7000     0.13422396573317902\n",
      "8000     0.13512390728713944\n",
      "9000     0.13715997306298877\n",
      "Epoch 2 Average Loss:  0.13738820332325996\n",
      "1000     0.12469653276726604\n",
      "2000     0.12465826020389795\n",
      "3000     0.12509883635366956\n",
      "4000     0.12522506265062838\n",
      "5000     0.12536556483358144\n",
      "6000     0.12545606596271197\n",
      "7000     0.12599340211280755\n",
      "8000     0.1270587191199884\n",
      "9000     0.12917863206275634\n",
      "Epoch 3 Average Loss:  0.1296509340494871\n",
      "1000     0.11557090275548398\n",
      "2000     0.11727688533160835\n",
      "3000     0.11867342106501261\n",
      "4000     0.11984943940863013\n",
      "5000     0.12034808116555214\n",
      "6000     0.12053787832458814\n",
      "7000     0.12118073540713106\n",
      "8000     0.12224775163317099\n",
      "9000     0.1244013633703192\n",
      "Epoch 4 Average Loss:  0.12500137217976154\n",
      "1000     0.10871651257760823\n",
      "2000     0.11144929803628474\n",
      "3000     0.1131993451503416\n",
      "4000     0.11500641189794987\n",
      "5000     0.11566433093398809\n",
      "6000     0.11591830887583394\n",
      "7000     0.11671180373270597\n",
      "8000     0.11782969692209735\n",
      "9000     0.12007991161528561\n",
      "Epoch 5 Average Loss:  0.1208307593613863\n",
      "1000     0.10428778141923249\n",
      "2000     0.10760090459603816\n",
      "3000     0.10997216573481758\n",
      "4000     0.11159857163950801\n",
      "5000     0.1122044205494225\n",
      "6000     0.11260692305117846\n",
      "7000     0.11348225925277386\n",
      "8000     0.11465413572220132\n",
      "9000     0.11690780063346028\n",
      "Epoch 6 Average Loss:  0.11769263333380223\n",
      "1000     0.10117012042738498\n",
      "2000     0.10454681939724833\n",
      "3000     0.1077025989027073\n",
      "4000     0.10839685043180361\n",
      "5000     0.10897580419592559\n",
      "6000     0.10943886289155731\n",
      "7000     0.11036201275991542\n",
      "8000     0.11162869943352416\n",
      "9000     0.11390026053289573\n",
      "Epoch 7 Average Loss:  0.114755286571756\n",
      "1000     0.09869011586718261\n",
      "2000     0.1025395481986925\n",
      "3000     0.10546403578172127\n",
      "4000     0.1058883008849807\n",
      "5000     0.10645078386627138\n",
      "6000     0.10692109688526641\n",
      "7000     0.10787100251285094\n",
      "8000     0.10910121293924749\n",
      "9000     0.11140654331487086\n",
      "Epoch 8 Average Loss:  0.11231905074380338\n",
      "1000     0.09731452443450689\n",
      "2000     0.10221889978460967\n",
      "3000     0.10374703330049913\n",
      "4000     0.1041875079581514\n",
      "5000     0.10477899984642863\n",
      "6000     0.10519090201891959\n",
      "7000     0.10608242218223002\n",
      "8000     0.1072620036082808\n",
      "9000     0.1095547341056582\n",
      "Epoch 9 Average Loss:  0.11044787716101855\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    for j in range(len(glove)):\n",
    "        x_var = torch.tensor(list(glove.values())[j]).squeeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        xpred_var = net(x_var)\n",
    "        loss = criterion(xpred_var, x_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # running loss\n",
    "        epoch_loss += loss.item()\n",
    "        if j%1000 == 0 and j > 0:\n",
    "            print(j, \"   \", epoch_loss / j)\n",
    "    print(f\"Epoch {epoch} Average Loss:  {epoch_loss / 10000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), \"data/7_sparse_glove.pt\")\n",
    "# net.load_state_dict(torch.load(\"data/very_sparse_glove.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2sparse(word):\n",
    "    return net.encode(torch.tensor(glove[word])).squeeze().detach().numpy()\n",
    "\n",
    "def vec_dist(vec1, vec2):\n",
    "    diff = vec1 - vec2\n",
    "    return np.sqrt(np.dot(diff, diff))\n",
    "\n",
    "def sparse_similarity(vec1, vec2):\n",
    "    count = 0\n",
    "    for index in range(len(vec1)):\n",
    "        if vec1[index] != 0 and vec2[index] != 0:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def show_sparse(vec):\n",
    "    output = {}\n",
    "    for index in range(len(vec)):\n",
    "        if vec[index] != 0:\n",
    "            output[index] = vec[index]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
